import yfinance as yf
from GoogleNews import GoogleNews
import pandas as pd
from datetime import datetime, timedelta
import trafilatura
from urllib.parse import urlparse, parse_qs


class DataFetcher:
    def __init__(self, ticker="NVDA"):
        self.ticker = ticker
        self.stock = yf.Ticker(ticker)

    def get_stock_data(self, period="5d", interval="1h"):
        df = self.stock.history(period=period, interval=interval)
        return df

    def get_market_status(self):
        todays_data = self.stock.history(period="1d")
        last_close = todays_data["Close"].iloc[-1] if not todays_data.empty else 0

        info = self.stock.info
        current_price = info.get(
            "currentPrice", info.get("regularMarketPrice", last_close)
        )

        change_percent = (
            ((current_price - last_close) / last_close) * 100 if last_close else 0
        )

        return {
            "last_close": last_close,
            "current_price": current_price,
            "change_percent": change_percent,
        }

    def get_recent_news(self, days=3):
        googlenews = GoogleNews(lang="en", region="US")
        googlenews.set_period(f"{days}d")
        googlenews.search(f"{self.ticker} stock")
        results = googlenews.result()

        news_list = []
        for news in results[:5]:
            news_list.append(f"- {news['title']} (Source: {news['media']})")

        return "\n".join(news_list)

    def clean_google_url(self, url):
        """à¸¥à¹‰à¸²à¸‡ Tracking Params à¸‚à¸­à¸‡ Google à¸­à¸­à¸à¸ˆà¸²à¸ URL"""
        try:
            # à¸à¸£à¸“à¸µ 1: à¹€à¸›à¹‡à¸™ Link redirect à¸‚à¸­à¸‡ google (google.com/url?q=...)
            if "google.com/url" in url:
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                if "q" in params:
                    url = params["q"][0]

            # à¸à¸£à¸“à¸µ 2: à¸¡à¸µ params &ved, &usg à¸•à¹ˆà¸­à¸—à¹‰à¸²à¸¢ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹ƒà¸™ log à¸‚à¸­à¸‡à¸„à¸¸à¸“)
            # à¸•à¸±à¸”à¸—à¸´à¹‰à¸‡à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆà¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸«à¸¡à¸²à¸¢ & à¸•à¸±à¸§à¹à¸£à¸à¸—à¸µà¹ˆà¹€à¸ˆà¸­à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸ˆà¸š URL à¸›à¸à¸•à¸´
            # à¸§à¸´à¸˜à¸µà¸šà¹‰à¸²à¸™à¹† à¹à¸•à¹ˆà¹„à¸”à¹‰à¸œà¸¥à¸„à¸·à¸­ split à¹€à¸­à¸²à¹à¸„à¹ˆà¸ªà¹ˆà¸§à¸™à¸«à¸™à¹‰à¸²
            if "&ved=" in url:
                url = url.split("&ved=")[0]
            if "&usg=" in url:
                url = url.split("&usg=")[0]

            return url
        except:
            return url

    def get_news_with_content(self, days=3, limit=5):
        print(f"ðŸ•µï¸â€â™‚ï¸ Searching news for {self.ticker}...")
        googlenews = GoogleNews(lang="en", region="US")
        googlenews.set_period(f"{days}d")
        # encode=True à¸Šà¹ˆà¸§à¸¢à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸ à¸²à¸©à¸²à¹à¸›à¸¥à¸à¹† à¹„à¸”à¹‰à¸šà¹‰à¸²à¸‡ à¹à¸•à¹ˆà¸–à¹‰à¸² Error à¸šà¹ˆà¸­à¸¢à¸¥à¸­à¸‡à¹€à¸­à¸²à¸­à¸­à¸à¹„à¸”à¹‰
        googlenews.search(f"{self.ticker} stock")
        results = googlenews.result()

        news_data = []
        count = 0

        for news in results:
            if count >= limit:
                break

            # --- à¹€à¸£à¸µà¸¢à¸à¹ƒà¸Šà¹‰à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸¥à¹‰à¸²à¸‡à¸¥à¸´à¸‡à¸à¹Œà¸•à¸£à¸‡à¸™à¸µà¹‰ ---
            raw_url = news["link"]
            url = self.clean_google_url(raw_url)
            # ----------------------------------

            title = news["title"]

            print(f"Processing: {url}")  # Print à¸”à¸¹à¸§à¹ˆà¸²à¸¥à¸´à¸‡à¸à¹Œà¸ªà¸°à¸­à¸²à¸”à¸«à¸£à¸·à¸­à¸¢à¸±à¸‡

            try:
                # à¹€à¸žà¸´à¹ˆà¸¡ config à¹ƒà¸«à¹‰ trafilatura à¹€à¸™à¸µà¸¢à¸™à¸‚à¸¶à¹‰à¸™
                downloaded = trafilatura.fetch_url(url)

                if downloaded:
                    content = trafilatura.extract(downloaded)

                    if content and len(content) > 100:
                        news_data.append(
                            {"title": title, "url": url, "content": content[:3000]}
                        )
                        count += 1
                else:
                    print(f"Empty response from {url}")

            except Exception as e:
                print(f"Failed to scrape {url}: {e}")
                continue

        return news_data
